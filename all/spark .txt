
一、几种部署模式


二、saprk core
  2.1、spark context 初始化
    2.1.1 listenerBus 用于事件同步

    2.1.2、createDriverEnv
            创建 SparkEnv for a driver or an executor
        2.2.1、 rpcEnv 通信框架
            1.3 之前 全部使用 akka, 1.6: rpcEnv 默认使用 netty; actorSystem 依然使用 akka
            此处使用了 工厂方法: getRpcEnvFactory   根据 spark.rpc 来获取

        2.2.2、actorSystem
            通信协议

        2.2.3、 serializer、closureSerializer 加载序列化类器
            serializer 序列化数据, 用反射机制 根据 spark.serializer 来加载序列化类器
            closureSerializer: 序列化闭包及 taskset

        2.2.4、shuffleManager  spark.shuffle.manager
            val shortShuffleMgrNames = Map(
              "hash" -> "org.apache.spark.shuffle.hash.HashShuffleManager",
              "sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager",
              "tungsten-sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager")


        2.2.5、 memoryManager 内存管理
            2.2.5.1、  1.6 之前 用  StaticMemoryManager
                1.6 之后 用  UnifiedMemoryManager

                spark.memory.useLegacyMode 是否使用 遗留下来的模型(StaticMemoryManager)

             2.2.5.2、 ExecutorAllocationManager
                spark.dynamicAllocation.enabled
                Start方法将 ExecutorAllocationListener 加入到listenerBus中，ExecutorAllocationListener通过监听listenerBus里的事件，动态添加，删除Executor。
                并且通过Thread不断添加Executor，遍历Executor，将超时的Executor杀掉并移除


        2.2.6、 注册或创建 blockManagerMaster、blockManager
                创建 blockTransferService
                然后创建 blockManagerMaster 和 blockManager

        2.2.7、 cacheManager
                根据blockManager, 最后创建 cacheManager
        2.2.8、mapOutputTracker
            Driver: MapOutputTrackerMaster
            ornot: MapOutputTrackerWorker


    2.3、 _heartbeatReceiver 心跳接收器等


    2.4、 taskScheduler、schedulerBackend
        根据不同的 部署模式创建 一下 taskScheduler、schedulerBackend
        用 TaskSchedulerImpl 创建
        通过 SparkDeploySchedulerBackend 创建 schedulerBackend

        调用 taskScheduler的 initialize 关联 schedulerBackend, 并初始化, 建立调度池 (spark.scheduler.mode)
        然后调用 taskScheduler.start(), 里面调用 backend.start()

        backend 同时启动一个 appclient  向 master 注册 application, 并与master保持通信

        启动一个固定频率的线程池,然后 启动线程 固定周期调度 需要 运行的 task

        taskScheduler task 的资源 申请 和 计算 , 负责执行


    2.5、 _dagScheduler
        2.5.1、
            初始化 dagScheduler, 会启动一个 DAGSchedulerEventProcessLoop 事件处理循环器, 处理提交上来的事件

            _dagScheduler 关联 taskScheduler
            action 触发方法,runjob方法 来 submint job, 将job 封装成 JobSubmitted(DAGSchedulerEvent接口), 提交到 eventProcessLoop


            private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
                case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
                  dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)

                case StageCancelled(stageId) =>
                  dagScheduler.handleStageCancellation(stageId)

                case JobCancelled(jobId) =>
                  dagScheduler.handleJobCancellation(jobId)

                case ExecutorAdded(execId, host) =>
                  dagScheduler.handleExecutorAdded(execId, host)

                case ExecutorLost(execId) =>
                  dagScheduler.handleExecutorLost(execId, fetchFailed = false)

                case completion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =>
                  dagScheduler.handleTaskCompletion(completion)

                case TaskSetFailed(taskSet, reason, exception) =>
                  dagScheduler.handleTaskSetFailed(taskSet, reason, exception)

                case ResubmitFailedStages =>
                  dagScheduler.resubmitFailedStages()
              }

        2.5.2、dagScheduler 处理 task 的本地性
            所谓的数据本地性是指：在计算时，数据本身已经在内存中或者利用已有缓存无需计算的方式获取数据。
                1、如果 已经缓存， 则返回 cache locations
                2、调用 rdd 的 preferredLocations 判断，如果 进行过 进行过 checkPoint ， 则 getPreferredLocations(split) 获取
                3、如果没有 checkPoint， 则 根据具体 的rdd 特性来获取他的本地性
                4、如果 是 窄依赖， 则通过 递归 寻找父RDD对应的partition的位置信息


            ShuffleMapStage -> ShuffleMapTask
            ResultStage -> ResultTask
                partitionsToCompute.map { id =>
                    val locs = taskIdToLocations(id)
                    val part = stage.rdd.partitions(id)
                    new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
                      taskBinary, part, locs, stage.internalAccumulators)
                  }
            将 tasks 封装成 TaskSet ，由 taskScheduler 提交， 由 taskSchedulerbceked 调度运行
                taskScheduler.submitTasks(new TaskSet(tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))



        2.5.3、stage 划分算法
            StageInfo -> task -> TaskSet


    2.6 启动 指标系统 metricsSystem.start()


    2.7、blockManager
        blockmanagermaster: driver--》dagscheldur-》blockmanagermaster-》blockmanagerinfo-》bolckstatus
        blockmanager: diststroe、memeorystore、blocktransferserver、 connectionmaster
               memeorystore 的 unrollsafely()展开内存, 调用  ensurefreespace() 释放内存


三、主备切换机制

  2.1、主备切换机制
  2.2、注册机制
  2.3、状态改变机制
  2.4、资源调度机制
  2.5、work资源分配










    end、spark性能调优
    20、spark 应用内存溢出发生在什么情况下?哪些算子会产生内存溢出?如何定位?如何解决?




    1、task 的本地性
        1、如果 已经缓存， 则返回 cache locations
        2、调用 rdd 的 preferredLocations 判断，如果 进行过 进行过 checkPoint ， 则 getPreferredLocations(split) 获取
        3、如果没有 checkPoint， 则 根据具体 的rdd 特性来获取他的本地性
        4、如果 是 窄依赖， 则通过 递归 寻找父RDD对应的partition的位置信息



    shufflemaptask
    resulttask


    shufflemanager
        shufflewriter
        shufflereader
        mapstatus


    mapoutputtracker  来去数据




    shuffleClient 负责 远程的数据的传输， 与 blockTransferService 作用相同
        * shuffleClient 负责与远程 excutor的链接， 1.3版本为： ConnectionManager
    blockTransferService





四、spark 调优
    1、序列化,  1.6 默认使用 java 原生类库 , cdh 调优默认 kryo 序列化
    2、最好 用简单的 数据类型, 避免 嵌套对象, 用int 替代 string
    3、多次迭代的 rdd , 要catch 或chekpint
    4、提高并行度
        a、spark.defalut.parallelism
        b、reduceByKey的算子指定partition的数量  rdd1.reduceByKey(_+_ ,10)
        c、val rdd3 = rdd1.join（rdd2）  rdd3里面partiiton的数量是由父RDD中最多的partition数量来决定，因此使用join算子的时候，增加父RDD中partition的数量
        d、spark.sql.shuffle.partitions //spark sql中shuffle过程中partitions的数量
    5、广播共享数据
    6、本地化级别
        spark.locality.wait(3000ms)
        spark.locatily.wait.node
        spark.locatily.process
        spark.locatily.rack


     7、shuffle 调优2
        a、spark.reducer.maxSizeInFlight            (重要)拉去大小         48M          reduce task 的buffer缓冲, 代表了每个reduce task 每次能够拉去的 map side 输入数据最大大小,如果内存充足,可以考虑加大大小, 从而减少网络传输次数, 提升性能
        b、spark.shuffle.blockTransferservice       (重要)                nettry       shuffle 过程中,传输数据的方式, 两种选项, netty 或nio, spark 1.2 开始, 默认就是inettry, 比较简单而且性能较高, spark1.5 开始nio就是过期的了,而且spark 1.6中会去除掉
        c、spark.shuffle.compress                   mapside 压缩          true         是否对map side 输出的文件进行压缩, 默认是启用压缩的, 压缩器是有 spark.io.compression.codec属性指定的, 默认是snappy压缩器, 该压缩器敲掉的是压缩速度, 而不是压缩率
        d、spark.shuffle.consolidaterFiles          (重要)                false        如果设置为true, 那么久会合并 map side 输出文件, 对于 reduce task数量 特别多的情况下,可以极大减少磁盘io开销, 提升性能
        e、spark.shuffle.file.buffer                (重要)mapside buffer  32k          map side task 的内存 buffer 大小, 写数据到磁盘文件之前, 会先保存在缓冲区中, 如果内存充足, 可以适当加大大小,从而减少 map side 磁盘io次数,提升性能
        f、spark.shuffle.io.maxRetries                                    3            重试次数
        f、spark.shuffle.io.retryWait                                     5S           重试间隔
        f、spark.shuffle.io.numConnectionsPerPeer                         1            机器之间可以重用网络连接, 主要用于在大型集群中减小网络连接的建立开销, 如果一个集群的机器很多,可以考虑增加这个值
        f、spark.shuffle.io.preferDirectBufs                              True         启用堆外内存, 可以避免shuffle 过程的频繁 gc, 如果堆外内存 诶长紧张, 则可以考虑关闭这个选项
        f、spark.shuffle.manager                    (重要)                 sort         shuffleManager,  Spark 1.5 以后有三种可选的, hash、sort和tungsten-sort, sort-basedshufflManager 会更搞笑使用内存, 并且避免产生大量的map side磁盘 文件, 从spark 1.2 开始就是默认的选项, tungsten-sort 与sort类似, 但是内存性能更高
        f、spark.shuffle.memoryFraction             (重要) reduce内存      0.2          如果spark.shuffle.spill 属性是true, 那么该选项生效, 代表了 executor 内存中, 用于进行shuffle reduce side 聚合的内存比例, 默认是20%, 如果内存充足,建议调高这个比例, 给reduce聚合更多内存,避免内存不足频繁读写磁盘
        f、spark.shuffle.service.enabled                                  false        启用外部shuffle服务 这个服务会安全的报酬shuffle过程中, executor写的磁盘文件, 因此executor即使挂掉也不要紧, 必须配合spark.dynamicAllocation.enabled 属性设置为true, 才会生效, 而且外部shuffle服务必须进行安装和启动, 才能使用这个属性
        f、spark.shuffle.service.port                                     7337         外部shuffle服务的端口号, 具体解释同上
        f、spark.shuffle.sort.bypassMergeThreshold  (重要)不排序            200          对于 sort-based ShuffleManager, 如果没有进行 Map side 聚合 而且reduce task 数量少于这个值,那么久不会进行排序, 如果你使用sort shuffleManager, 而且不需要排序, 那么可以考虑将这个值加大, 直到比你指定的所有task数量都大, 以避免进行额外的sort, 从而提高性能
        f、spark.shuffle.spill                                            true         当 reduce side 的聚合内存使用量超过了 spark.shuffle.memoryFranction 指定的比例时, 就进行磁盘的溢写操作
        f、spark.shuffle.spill.compress                                   true         同上, 进行磁盘溢写时, 是否进行文件压缩, 使用 spark.io.compression.codec 属性指定的压缩器,默认是snappy, 速度优先



五 spark 2.0
    提升cpu 但无法提升 io
    1、新特性
        a、dataset
        b、sparkSession
        c、spark sql 支持 sql 2003标准l, 并支持 子查询
        d、支持内存和程序运行的堆外内存管理, 支持 hive 风格的bucket 表
        e、通过whole-stage code generation (全流程代码生成) 技术 提升 spaksql 的性能
        f、通过 vectorization 技术提升 parquet 文件的小苗推土量,
        g、提升 orc 文件的 读写性能, 提升 catalyst 查询优化器的性能, 通过 native 实现 窗口函数, 提升性能
        h、structured streaming ,
            基于 sparksql 和 catalyst 引擎构建
            支持 使用 dataframe 风格的api 进行流失计算
            catalyst 引擎能够对执行计划进行优化

        i、完全移除 akka 的依赖, 移除了 hash-based shuffle manager 不支持
        j、scala2.11 替代了 scala2.10

    2、whole-stage code generation
        旧的 模型是 volcano iterator mode
        让spark 作为编译器来运行: 在运行时动态生成代码,将代码打包到一个函数中, 有点:
            a、避免多次 virtual function call,
            b、而且还可以 通过 cpu register 来读写中间数据,而不是通过cpu cache来读写数据
            c、loop unrolling 和 SIMD: scan 变成 for循环。  编译器通常可以自动对for 循环进行 unrolling
                loop unrolling: 编译器对简单for 循环 进行 展开, 从而提升性能
                simd:  在for展开时会生成 SIMD 指令, 让cpu一次处理多条数据
                cpu 也包含一些其他的特性, 比如pipelining、prefeetching, 指令 reordring可以让for循环执行性能更高, 但是都无法再负责的函数调用场景中施展


    3、vectorization
        将多条数据通过面向列的方式来组织成一个一个batch, 然后对一个batch中的数据来进行迭代处理, 每次next函数调用都返回一个batch的数据
            a、减少virtual function call
            b、loop unrolling
            但无法 时用 cpu register

    4、structure streaming
        1、保证与批处理作业的强一致性
        2、与存储系统进行事务性的种鸽
        3、与 saprk 的其他部分进行无缝整合
        4、支持 event-time
        5、一次且仅一次

        input batch -> inputtable(trigger interval 1s) ->  resulttable -> sink(complete mode、append mode、update mode)



    5、sparkSession
        import org.apache.spark.sql.SparkSession
        val spark: SparkSession = SparkSession.builder
          .appName("My Spark Application")  // optional and will be autogenerated if not specified
          .master("local[*]")               // avoid hardcoding the deployment environment
          .enableHiveSupport()              // self-explanatory, isn't it?
          .config("spark.sql.warehouse.dir", "target/spark-warehouse")
          .getOrCreate

    6、dataset
        dataset 与 rdd 比较类似, 但是非常重要的一点不同是, rdd 的序列化是给予java序列化或 kryo的, 而dataset的序列化机制是基于一种特殊的 encoder,来将对象进行高效序列化, 以进行高性能处理或者通过网络进行传输。
            dataset 除了encoder, 也同时支持java 序列化, 但是encdoer 的特点在于动态的代码生成, 同时提供一种特殊的数据格式, 来让spark 不将对象记性反序列化, 即可直接基于二进制数据执行一些常见的啊哦做, 比如filter、sort、hash等




六、spark streaming
    1、streaming 的背压
        1.1、Spark 1.5 版本之前:
            receivers: spark.streaming.receiver.maxRate: 每秒最大可以接收的记录的数据
            Direct: spark.streaming.kafka.maxRatePerPartition : 每次作业中每个 Kafka 分区最多读取的记录条数
            1、问题
                a、我们需要事先估计好集群的处理速度以及消息数据的产生速度
                b、这两种方式需要人工参与，修改完相关参数之后，我们需要手动重启 Spark Streaming 应用程序；
        1.2、Spark 1.5 版本:
            1、为了实现自动调节数据的传输速率，在原有的架构上新增了一个名为 RateController 的组件，这个组件继承自 StreamingListener，其监听所有作业的 onBatchCompleted 事件，
                并且基于 processingDelay 、schedulingDelay 、当前 Batch 处理的记录条数以及处理完成事件来估算出一个速率；这个速率主要用于更新流每秒能够处理的最大记录的条数。速率估算器（RateEstimator）
            2、InputDStreams 内部的 RateController 里面会存下计算好的最大速率，这个速率会在处理完 onBatchCompleted 事件之后将计算好的速率推送到 ReceiverSupervisorImpl，-> blockGenerator。
            3、如果用户配置了 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition，那么最后到底接收多少数据取决于三者的最小值。也就是说每个接收器或者每个 Kafka 分区每秒处理的数据不会超过 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition 的值。

        1.3、启动背压机制
            spark.streaming.backpressure.enabled=true
            spark.streaming.backpressure.initialRate： 启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置。

    2、 UpdateStateByKey 和 MapWithState
        a、MapWithState 1.6 引入:
            StateSpec.function 实现, 注意这里不是直接传入的mappingFunc函数，而是一个StateSpec 的对象
            1、返回的都是有update的数据，
            2、若要获取所有的状态在mapWithState之后调用stateSnapshots即可。
            3、若要清除某个key的状态，可在自定义的方法中调用state.remove()。


        b、UpdateStateByKey:
            1、每次调用updateStateByKey都会将旧的状态RDD和当前batch的RDD进行co-group来得到一个新的状态RDD
            2、每次都会返回所有数据


    3、Spark Streaming实时计算海量用户UV
        Redis
            a、Bitmap方案
                插入setbit key offset value//获取getbit key offset//计数BITCOUNT key [start] [end], 这里offset最大值就是2^32.
                用户id 小 2^32, 放入  bit中 , 用countbit 统计结果

            b、HyperLogLog 可以接受多个元素作为输入，并给出输入元素的基数估算值：
                • 基数：集合中不同元素的数量。比如 {'apple', 'banana', 'cherry', 'banana', 'apple'} 的基数就是 3 。
                • 估算值：算法给出的基数并不是精确的，可能会比实际稍微多一些或者稍微少一些，但会控制在合
                  优点是，即使输入元素的数量或者体积非常非常大，计算基数所需的空间总是固定的、并且是很小的。
                a、添加
                    PFADD key element [element ...]
                    PFCOUNT key [key ...]
